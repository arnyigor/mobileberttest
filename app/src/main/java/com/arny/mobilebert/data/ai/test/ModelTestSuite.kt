package com.arny.mobilebert.data.ai.test

import com.arny.mobilebert.data.ai.models.TestCase
import com.arny.mobilebert.data.ai.models.TestCategory
import com.arny.mobilebert.data.ai.models.TokenizeResult

class ModelTestSuite {
    // 1. –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ç–µ—Å—Ç—ã
    val technicalTests = listOf(
        // –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
        TestCase(
            text = "MVVM –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è",
            expectedTokens = 3,
            expectedImportantWords = setOf("mvvm", "–∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞"),
            category = TestCategory.TECHNICAL
        ),
        TestCase(
            text = """
            Clean Architecture –≤ Android –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–∏:
            - Presentation layer (MVVM)
            - Domain layer (Use Cases)
            - Data layer (Repository)
            """.trimIndent(),
            expectedTokens = 12,
            expectedImportantWords = setOf("clean", "architecture", "mvvm", "repository"),
            category = TestCategory.TECHNICAL
        ),

        // Kotlin-—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ
        TestCase(
            text = """
            –ö–æ—Ä—É—Ç–∏–Ω—ã –≤ Kotlin:
            suspend fun loadData() = coroutineScope {
                val result = async { api.getData() }
                result.await()
            }
            """.trimIndent(),
            expectedTokens = 15,
            expectedImportantWords = setOf("–∫–æ—Ä—É—Ç–∏–Ω—ã", "suspend", "coroutinescope", "async"),
            category = TestCategory.TECHNICAL
        ),

        // Android –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
        TestCase(
            text = """
            Lifecycle-aware ViewModels –∏—Å–ø–æ–ª—å–∑—É—é—Ç StateFlow:
            private val _uiState = MutableStateFlow<UiState>(UiState.Initial)
            val uiState: StateFlow<UiState> = _uiState.asStateFlow()
            """.trimIndent(),
            expectedTokens = 18,
            expectedImportantWords = setOf("lifecycle", "viewmodels", "stateflow"),
            category = TestCategory.TECHNICAL
        )
    )

    // 2. –ü–æ–∏—Å–∫–æ–≤—ã–µ —Ç–µ—Å—Ç—ã
    val searchTests = listOf(
        // –ü—Ä–æ—Å—Ç–æ–π –ø–æ–∏—Å–∫
        TestCase(
            text = "–∫–∞–∫ —Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å MVVM –≤ Android –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–∏",
            expectedImportantWords = setOf("mvvm", "android", "—Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å"),
            category = TestCategory.SEARCH
        ),

        // –°–ª–æ–∂–Ω—ã–π –ø–æ–∏—Å–∫
        TestCase(
            text = "–º–µ–¥–ª–µ–Ω–Ω–∞—è –ø—Ä–æ–∫—Ä—É—Ç–∫–∞ —Å–ø–∏—Å–∫–∞ —Å –∫–∞—Ä—Ç–∏–Ω–∫–∞–º–∏ –≤ RecyclerView",
            expectedImportantWords = setOf("–º–µ–¥–ª–µ–Ω–Ω–∞—è", "recyclerview", "—Å–ø–∏—Å–æ–∫", "–∫–∞—Ä—Ç–∏–Ω–∫–∏"),
            category = TestCategory.SEARCH
        ),

        // –ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –ø–æ–∏—Å–∫
        TestCase(
            text = "—É—Ç–µ—á–∫–∏ –ø–∞–º—è—Ç–∏ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π",
            expectedImportantWords = setOf("—É—Ç–µ—á–∫–∏", "–ø–∞–º—è—Ç—å", "–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è"),
            category = TestCategory.SEARCH
        )
    )

    // 3. –°–º–µ—à–∞–Ω–Ω—ã–µ —è–∑—ã–∫–æ–≤—ã–µ —Ç–µ—Å—Ç—ã
    val mixedLanguageTests = listOf(
        TestCase(
            text = """
            @Composable
            fun MainScreen() {
                // –û—Å–Ω–æ–≤–Ω–æ–π —ç–∫—Ä–∞–Ω –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è
                Column(modifier = Modifier.fillMaxSize()) {
                    TopBar(title = "–ì–ª–∞–≤–Ω–∞—è")
                    ContentList(items = uiState.items)
                }
            }
            """.trimIndent(),
            expectedImportantWords = setOf("composable", "mainscreen", "column", "topbar"),
            category = TestCategory.MIXED_LANGUAGE
        ),

        TestCase(
            text = "git commit -m '–ò—Å–ø—Ä–∞–≤–ª–µ–Ω –∫—Ä–∞—à –ø—Ä–∏ deep linking'",
            expectedImportantWords = setOf("crash", "deep", "linking"),
            category = TestCategory.MIXED_LANGUAGE
        )
    )

    // 4. –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫
    val errorTests = listOf(
        TestCase(
            text = """
            E/AndroidRuntime: FATAL EXCEPTION: main
                Process: com.example.app, PID: 1234
                java.lang.IllegalStateException: Fragment UserProfileFragment not attached to an activity
                at androidx.fragment.app.Fragment.requireActivity(Fragment.java:805)
            """.trimIndent(),
            expectedImportantWords = setOf("fatal", "exception", "fragment", "illegalstateexception"),
            category = TestCategory.ERROR_HANDLING
        ),

        TestCase(
            text = "Network error: Unable to resolve host 'api.example.com'",
            expectedImportantWords = setOf("network", "error", "host"),
            category = TestCategory.ERROR_HANDLING
        )
    )

    // 5. –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–ª—É—á–∞–∏
    val specialTests = listOf(
        TestCase(
            text = "compileSdk = 34",
            expectedTokens = 3,
            category = TestCategory.SPECIAL_CASES
        ),

        TestCase(
            text = "implementation 'androidx.compose.ui:ui:1.5.0'",
            expectedImportantWords = setOf("implementation", "compose", "ui"),
            category = TestCategory.SPECIAL_CASES
        ),

        TestCase(
            text = "üöÄ New Release v2.0.0 üéâ",
            expectedImportantWords = setOf("release"),
            category = TestCategory.SPECIAL_CASES
        )
    )

    // 6. –¢–µ—Å—Ç—ã –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
    val performanceTests = listOf(
        TestCase(
            text = generateLongText(1000),
            expectedProcessingTime = 500,
            category = TestCategory.PERFORMANCE
        ),
        TestCase(
            text = generateRepeatingText("Android Development", 100),
            expectedTokens = 200,
        )
    )

    // 7. –¢–µ—Å—Ç—ã —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏
    val tokenizationTests = listOf(
        TestCase(
            text = "—Ç–µ–∫—Å—Ç —Å –ø—Ä–æ–±–µ–ª–∞–º–∏",
            expectedTokens = 5,
            expectedTokenSequence = listOf(
                "[CLS]", "—Ç–µ–∫—Å—Ç", "—Å", "–ø—Ä–æ–±–µ–ª–∞–º–∏", "[SEP]"
            ),
            category = TestCategory.BASE_TOKENIZATION
        ),
        TestCase(
            text = "preprocessing",
            expectedTokens = 3,
            expectedTokenSequence = listOf(
                "[CLS]", "preprocessing", "[SEP]"
            ),
            category = TestCategory.BASE_TOKENIZATION
        ),
        TestCase(
            text = "embeddings",
            expectedTokens = 3,
            expectedTokenSequence = listOf(
                "[CLS]", "embeddings", "[SEP]"
            ),
            category = TestCategory.BASE_TOKENIZATION
        )
    )

    // –ù–æ–≤—ã–µ —Ç–µ—Å—Ç–æ–≤—ã–µ —Å–ª—É—á–∞–∏
    val extendedTokenizationTests = listOf(
        TestCase(
            text = "–§–∏–ª—å–º \"–ù–∞—á–∞–ª–æ\" —Å –õ–µ–æ–Ω–∞—Ä–¥–æ –î–∏ –ö–∞–ø—Ä–∏–æ",
            expectedTokens = 9,
            expectedTokenSequence = listOf(
                "[CLS]", "—Ñ–∏–ª—å–º", "\"", "–Ω–∞—á–∞–ª–æ", "\"", "—Å", "–ª–µ–æ–Ω–∞—Ä–¥–æ", "–¥–∏", "–∫–∞–ø—Ä–∏–æ", "[SEP]"
            ),
            expectedInputIds = listOf(2L, 5L, 6L, 7L, 6L, 8L, 9L, 10L, 11L, 3L),
            expectedMaskIds = listOf(1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L),
            expectedTypeIds = listOf(0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L),
            expectedImportantWords = setOf("–Ω–∞—á–∞–ª–æ", "–ª–µ–æ–Ω–∞—Ä–¥–æ", "–¥–∏", "–∫–∞–ø—Ä–∏–æ"),
            category = TestCategory.EXTENDED_TOKENIZATION,
            description = "–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–∞ —Å –Ω–∞–∑–≤–∞–Ω–∏–µ–º —Ñ–∏–ª—å–º–∞ –∏ –∏–º–µ–Ω–µ–º –∞–∫—Ç–µ—Ä–∞"
        ),
        TestCase(
            text = "–ö–æ–º–µ–¥–∏—è —Å –ê–¥–∞–º–æ–º –°—ç–Ω–¥–ª–µ—Ä–æ–º",
            expectedTokens = 6,
            expectedTokenSequence = listOf(
                "[CLS]", "–∫–æ–º–µ–¥–∏—è", "—Å", "–∞–¥–∞–º–æ–º", "—Å—ç–Ω–¥–ª–µ—Ä–æ–º", "[SEP]"
            ),
            expectedInputIds = listOf(2L, 12L, 8L, 13L, 14L, 3L),
            expectedMaskIds = listOf(1L, 1L, 1L, 1L, 1L, 1L),
            expectedTypeIds = listOf(0L, 0L, 0L, 0L, 0L, 0L),
            expectedImportantWords = setOf("–∞–¥–∞–º–æ–º", "—Å—ç–Ω–¥–ª–µ—Ä–æ–º"),
            category = TestCategory.EXTENDED_TOKENIZATION,
            description = "–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–∞ —Å –∏–º–µ–Ω–µ–º –∞–∫—Ç–µ—Ä–∞"
        ),
        TestCase(
            text = "–†–µ–∂–∏—Å—Å–µ—Ä –ú–∞—Ä—Ç–∏–Ω –°–∫–æ—Ä—Å–µ–∑–µ",
            expectedTokens = 5,
            expectedTokenSequence = listOf(
                "[CLS]", "—Ä–µ–∂–∏—Å—Å–µ—Ä", "–º–∞—Ä—Ç–∏–Ω", "—Å–∫–æ—Ä—Å–µ–∑–µ", "[SEP]"
            ),
            expectedImportantWords = setOf("–º–∞—Ä—Ç–∏–Ω", "—Å–∫–æ—Ä—Å–µ–∑–µ"),
            category = TestCategory.EXTENDED_TOKENIZATION,
            description = "–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–∞ —Å –∏–º–µ–Ω–µ–º —Ä–µ–∂–∏—Å—Å–µ—Ä–∞"
        ),
        TestCase(
            text = "–î—Ä–∞–º–∞ –æ –ª—é–±–≤–∏ –∏ –ø—Ä–µ–¥–∞—Ç–µ–ª—å—Å—Ç–≤–µ",
            expectedTokens = 7,
            expectedTokenSequence = listOf(
                "[CLS]", "–¥—Ä–∞–º–∞", "–æ", "–ª—é–±–≤–∏", "–∏", "–ø—Ä–µ–¥–∞—Ç–µ–ª—å—Å—Ç–≤–µ", "[SEP]"
            ),
            expectedImportantWords = setOf("–¥—Ä–∞–º–∞", "–ª—é–±–≤–∏", "–ø—Ä–µ–¥–∞—Ç–µ–ª—å—Å—Ç–≤–µ"),
            category = TestCategory.EXTENDED_TOKENIZATION,
            description = "–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–∞ —Å –æ–ø–∏—Å–∞–Ω–∏–µ–º —Ñ–∏–ª—å–º–∞"
        ),
        TestCase(
            text = "–§–∏–ª—å–º —Å —ç–ª–µ–º–µ–Ω—Ç–∞–º–∏ —Ñ–∞–Ω—Ç–∞—Å—Ç–∏–∫–∏ –∏ —ç–∫—à–µ–Ω–∞",
            expectedTokens = 8,
            expectedTokenSequence = listOf(
                "[CLS]", "—Ñ–∏–ª—å–º", "—Å", "—ç–ª–µ–º–µ–Ω—Ç–∞–º–∏", "—Ñ–∞–Ω—Ç–∞—Å—Ç–∏–∫–∏", "–∏", "—ç–∫—à–µ–Ω–∞", "[SEP]"
            ),
            expectedImportantWords = setOf("—Ñ–∞–Ω—Ç–∞—Å—Ç–∏–∫–∏", "—ç–∫—à–µ–Ω–∞"),
            category = TestCategory.EXTENDED_TOKENIZATION,
            description = "–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–∞ —Å –∂–∞–Ω—Ä–∞–º–∏ —Ñ–∏–ª—å–º–∞"
        ),
        TestCase(
            text = "–°–µ–º–µ–π–Ω—ã–π —Ñ–∏–ª—å–º —Å –î–∂—É–¥–∏ –î–µ–Ω—á",
            expectedTokens = 6,
            expectedTokenSequence = listOf(
                "[CLS]", "—Å–µ–º–µ–π–Ω—ã–π", "—Ñ–∏–ª—å–º", "—Å", "–¥–∂—É–¥–∏", "–¥–µ–Ω—á", "[SEP]"
            ),
            expectedImportantWords = setOf("–¥–∂—É–¥–∏", "–¥–µ–Ω—á"),
            category = TestCategory.EXTENDED_TOKENIZATION,
            description = "–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–∞ —Å –∏–º–µ–Ω–µ–º –∞–∫—Ç—Ä–∏—Å—ã"
        )
    )

    fun getAllTests(): List<Pair<String, List<TestCase>>> = listOf(
        "Technical Analysis" to technicalTests,
        "Search Capabilities" to searchTests,
        "Mixed Language Processing" to mixedLanguageTests,
        "Error Handling" to errorTests,
        "Special Cases" to specialTests,
        "Performance" to performanceTests,
        "Tokenization" to tokenizationTests
    )

    // –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏
    fun validateTokenization(
        result: TokenizeResult,
        testCase: TestCase
    ): List<String> {
        val errors = mutableListOf<String>()

        // –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ç–æ–∫–µ–Ω–æ–≤
        testCase.expectedTokens?.let { expected ->
            if (result.tokens.size != expected) {
                errors.add(
                    "Token count mismatch: expected $expected, got ${result.tokens.size}"
                )
            }
        }

        // –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Ç–æ–∫–µ–Ω–æ–≤
        testCase.expectedTokenSequence?.let { expected ->
            if (result.tokens != expected) {
                errors.add(
                    "Token sequence mismatch:\nexpected: $expected\ngot: ${result.tokens}"
                )
            }
        }

        // –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –º–∞—Å—Å–∏–≤–æ–≤
        if (result.inputIds.size != result.tokens.size ||
            result.maskIds.size != result.tokens.size ||
            result.typeIds.size != result.tokens.size
        ) {
            errors.add("Arrays size mismatch")
        }

        return errors
    }

    private fun generateLongText(words: Int): String {
        val vocabulary = listOf(
            "Android", "Kotlin", "development", "application", "architecture",
            "testing", "performance", "optimization", "implementation", "component"
        )
        return buildString {
            repeat(words) {
                append(vocabulary.random())
                append(" ")
            }
        }.trim()
    }

    private fun generateRepeatingText(base: String, times: Int): String {
        return buildString {
            repeat(times) {
                append(base)
                append(" ")
            }
        }.trim()
    }
}